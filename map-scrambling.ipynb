{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Scrambling\n",
    "\n",
    "This is an experiment to see how time and location data can be scrambled while still allowing querying based on proximity.\n",
    "\n",
    "\n",
    "## Sample Data\n",
    "\n",
    "First, lets source some open data from the City of Cape Town in order to generate sample location data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: taxi_routes; format: kml; length: 24602482\n",
      "  0 points, 300 paths with 119939 total points\n",
      "Name: fire_stations; format: kml; length: 10200\n",
      "  30 points, 0 paths with 0 total points\n",
      "74486 unique points including routes\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlencode, quote_plus\n",
    "import requests\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import hashlib\n",
    "import random\n",
    "import pprint\n",
    "import zipfile\n",
    "import pygeoif.geometry\n",
    "from fastkml import kml\n",
    "\n",
    "cache_dir = os.path.join(os.getcwd(), '_cache')\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)\n",
    "\n",
    "def make_cache_name(base_name, input_params, suffix):\n",
    "    input_str = pprint.pformat(sorted([(a, getattr(input_params, a)) for a in dir(input_params) if not a.startswith('_')]))\n",
    "    filename = '%s-%s.%s' % (base_name, hashlib.sha1(input_str.encode('utf-8')).hexdigest(), suffix)\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "# TODO: you need to get the City of Cape Town consent to download these files, as described on the home page URLs\n",
    "sample_data_sets = {\n",
    "    'taxi_routes': {'description': 'The file stores the spatial location of features (point, line, polygon) of taxi routes.',\n",
    "                    'license_url': 'https://web1.capetown.gov.za/web1/OpenDataPortal/Document/GetDocument/1',\n",
    "                    'home_page': 'https://web1.capetown.gov.za/web1/OpenDataPortal/DatasetDetail?DatasetName=Taxi%20routes',\n",
    "                    'url': 'http://cityapps.capetown.gov.za/sites/opendatacatalog/Documents/Taxi%20routes/Taxi_Routes.kmz',\n",
    "                    'skip': False,\n",
    "                   },\n",
    "    'fire_stations': {'description': 'Indicates the location of fire stations.',\n",
    "                    'license_url': 'https://web1.capetown.gov.za/web1/OpenDataPortal/Document/GetDocument/1',\n",
    "                    'home_page': 'https://web1.capetown.gov.za/web1/OpenDataPortal/DatasetDetail?DatasetName=Fire%20stations',\n",
    "                    'url': 'http://cityapps.capetown.gov.za/sites/opendatacatalog/Documents/Fire%20stations/Fire%20stations%202016.kmz',\n",
    "                     }\n",
    "}\n",
    "\n",
    "points = set()\n",
    "routes = []\n",
    "route_points = set()\n",
    "\n",
    "# read in all the data\n",
    "for data_set_name, data_set_attrs in sample_data_sets.items():\n",
    "    if data_set_attrs.get('skip', False):\n",
    "        continue\n",
    "    url = data_set_attrs['url']\n",
    "    document_name = url[url.rfind('/')+1:]\n",
    "    document_format = document_name[document_name.rfind('.')+1:]\n",
    "    actual_url = 'https://www.capetown.gov.za/_layouts/OpenDataPortalHandler/DownloadHandler.ashx?DocumentName=%s&DatasetDocument=%s' % (document_name, quote_plus(url))\n",
    "    url_hash = hashlib.sha256(actual_url.encode('UTF-8')).hexdigest()\n",
    "    cache_filename = os.path.join(cache_dir, '%s-%s' % (url_hash, document_name))\n",
    "    if os.path.exists(cache_filename):\n",
    "        with open(cache_filename, 'rb') as f:\n",
    "            data = f.read()\n",
    "    else:\n",
    "        data = requests.get(actual_url).content\n",
    "        with open(cache_filename, 'wb') as f:\n",
    "            f.write(data)\n",
    "    if document_format == 'kmz':\n",
    "        with zipfile.ZipFile(cache_filename) as kmz_zip:\n",
    "            with kmz_zip.open('doc.kml') as kml_file:\n",
    "                data = kml_file.read()\n",
    "        document_format = 'kml'\n",
    "    print(\"Name: %s; format: %s; length: %s\" % (data_set_name, document_format, len(data)))\n",
    "    k = kml.KML()\n",
    "    k.from_string(data)\n",
    "    count_points, count_paths, count_path_points = 0, 0, 0\n",
    "    for document in k.features():\n",
    "        for folder in document.features():\n",
    "            for placemark in folder.features():\n",
    "                geometry = placemark.geometry\n",
    "                if isinstance(geometry, pygeoif.geometry.Point):\n",
    "                    points.add((geometry.x, geometry.y))\n",
    "                    count_points += 1\n",
    "                elif isinstance(geometry, pygeoif.geometry.MultiLineString):\n",
    "                    # the taxi routes data has a single linestring per MultiLineString\n",
    "                    for i, linestring in enumerate(geometry.geoms):\n",
    "                        line_points = [(g.x, g.y) for g in linestring.geoms]\n",
    "                        routes.append(line_points)\n",
    "                        route_points.update(line_points)\n",
    "                        # TEMPORARY LIMITS\n",
    "                        if len(routes) > 300:\n",
    "                            break\n",
    "                        count_paths += 1\n",
    "                        count_path_points += len(linestring.geoms)\n",
    "                else:\n",
    "                    print(\"Unexpected type of geometry: %r\" % geometry)\n",
    "    print(\"  %d points, %d paths with %d total points\" % (count_points, count_paths, count_path_points))\n",
    "\n",
    "all_points = points.union(route_points)\n",
    "print(\"%d unique points including routes\" % len(all_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating time and location data\n",
    "\n",
    "Let's generate a sample set of simulated moving cellphone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample tracking data\n",
      "Tracked 5000 phones and generated 14403332 time-phone-location tuples, with 41286 trips between 14440 phone-locations\n"
     ]
    }
   ],
   "source": [
    "class gen_params:\n",
    "    PHONES = 5000\n",
    "    MINUTES = 60*24*14\n",
    "    SAMPLE_FREQUENCY = 7\n",
    "    SAMPLE_STDEV = 2\n",
    "    LOCATIONS_AVG = 2\n",
    "    LOCATIONS_STDEV = 4\n",
    "    TRIPS_PER_DAY_AVG = 1\n",
    "    TRIPS_PER_DAY_STDEV = 1.5\n",
    "    TRIP_TIME_STDEV = SAMPLE_FREQUENCY*10\n",
    "    TRIP_STDEV = 10/111111. # about 10 meters\n",
    "    WALK_STDEV = 2/111111. # about 2 meters\n",
    "    LOCATION_STDEV = 1000/111111. # about 1 km\n",
    "\n",
    "time_phone_location = []\n",
    "\n",
    "total_trips = 0\n",
    "total_locations = 0\n",
    "\n",
    "min_x, max_x, min_y, max_y = None, None, None, None\n",
    "\n",
    "def gen_data():\n",
    "    global total_trips\n",
    "    global total_locations\n",
    "    for phone_id in range(gen_params.PHONES):\n",
    "        trips_per_day = max(random.gauss(gen_params.TRIPS_PER_DAY_AVG, gen_params.TRIPS_PER_DAY_STDEV), 0.1)\n",
    "        location_count = max(int(random.gauss(gen_params.LOCATIONS_AVG, gen_params.LOCATIONS_STDEV)), 1)\n",
    "        location_bases = random.sample(points, location_count)\n",
    "        locations = [(x+random.gauss(0, gen_params.LOCATION_STDEV), y+random.gauss(0, gen_params.LOCATION_STDEV)) for (x, y) in location_bases]\n",
    "        minute = 0.0\n",
    "        location_n = random.randint(0, location_count-1)\n",
    "        total_locations += location_count\n",
    "        x, y = locations[location_n]\n",
    "        trips_taken = 0\n",
    "        in_trip = False\n",
    "        while minute < gen_params.MINUTES:\n",
    "            minute += random.gauss(gen_params.SAMPLE_FREQUENCY, gen_params.SAMPLE_STDEV)\n",
    "            if in_trip:\n",
    "                if trip_point >= len(trip):\n",
    "                    in_trip = False\n",
    "                    # get a different location to the last one\n",
    "                    if location_count > 1:\n",
    "                        new_location_n = random.randint(0, location_count-2)\n",
    "                        location_n = new_location_n + 1 if new_location_n >= location_n else new_location_n\n",
    "                    else:\n",
    "                        location_n = 0\n",
    "                    x, y = locations[location_n]\n",
    "                else:\n",
    "                    x, y = trip[trip_point]\n",
    "                    x += random.gauss(0, gen_params.TRIP_STDEV)\n",
    "                    y += random.gauss(0, gen_params.TRIP_STDEV)\n",
    "                    trip_point += 1\n",
    "            if not in_trip:\n",
    "                x += random.gauss(0, gen_params.WALK_STDEV)\n",
    "                y += random.gauss(0, gen_params.WALK_STDEV)\n",
    "                if (minute / 60*24) > (1/trips_per_day - trips_taken) + random.gauss(0, gen_params.TRIP_TIME_STDEV):\n",
    "                    in_trip, trip, trip_point = True, random.choice(routes), 0\n",
    "                    trips_taken += 1\n",
    "                    total_trips += 1\n",
    "            yield (minute, phone_id, (x, y))\n",
    "\n",
    "tpl_series_cache_filename = make_cache_name('time-phone-location', gen_params, 'csv')\n",
    "if not os.path.exists(tpl_series_cache_filename):\n",
    "    print(\"Generating sample tracking data\")\n",
    "    with open(tpl_series_cache_filename, 'w') as f:\n",
    "        series_writer = csv.writer(f)\n",
    "        records = 0\n",
    "        for minute, phone_id, (x, y) in gen_data():\n",
    "            series_writer.writerow((str(minute), str(phone_id), str(x), str(y)))\n",
    "            records += 1\n",
    "        print(\"Tracked %d phones and generated %d time-phone-location tuples, with %d trips between %d phone-locations\" % (gen_params.PHONES, records, total_trips, total_locations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sectoring and Anonymizing Time-Phone-Location Data\n",
    "\n",
    "We divide the location set into roughly square sectors.\n",
    "\n",
    "We then generate a sector ID for each location in the time-phone-location dataset\n",
    "\n",
    "Finally we store a hash of the time and sector ID, rather than the location, whilst still remembering sector adjacency for that time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating phone<->bucket maps\n",
      "5000 phones map to 6195196 buckets\n"
     ]
    }
   ],
   "source": [
    "class grid_params:\n",
    "    GRID_SIZE = 25/111111.  # a rough estimate\n",
    "    TIME_POCKET = 60 # minutes\n",
    "\n",
    "def distance(origin, destination):\n",
    "    # thanks https://gist.github.com/rochacbruno/2883505\n",
    "    lat1, lon1 = origin\n",
    "    lat2, lon2 = destination\n",
    "    radius = 6371 # km\n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "def coords_to_grid(x, y):\n",
    "    return int(x / grid_params.GRID_SIZE), int(y / grid_params.GRID_SIZE)\n",
    "\n",
    "min_x, max_x, min_y, max_y = None, None, None, None\n",
    "\n",
    "unique_track_points = set()\n",
    "unique_grid_points = set()\n",
    "\n",
    "def gen_grid():\n",
    "    global min_x, max_x, min_y, max_y\n",
    "    with open(tpl_series_cache_filename) as csv_file:\n",
    "        series_reader = csv.reader(csv_file)\n",
    "        for row in series_reader:\n",
    "            minute_s, phone_id_s, x_s, y_s = row\n",
    "            minute, phone_id, x, y = float(minute_s), int(phone_id_s), float(x_s), float(y_s)\n",
    "            unique_track_points.add((x, y))\n",
    "            if min_x is None or x < min_x:\n",
    "                min_x = x\n",
    "            if max_x is None or x > max_x:\n",
    "                max_x = x\n",
    "            if min_y is None or y < min_y:\n",
    "                min_y = y\n",
    "            if max_y is None or y > max_y:\n",
    "                max_y = y\n",
    "            yield (minute, phone_id, coords_to_grid(x, y))\n",
    "\n",
    "tpg_series_cache_filename = make_cache_name('time-phone-grid', grid_params, 'csv')\n",
    "if not os.path.exists(tpg_series_cache_filename):\n",
    "    print (\"Converting to grid\")\n",
    "    with open(tpg_series_cache_filename, 'w') as f:\n",
    "        series_writer = csv.writer(f)\n",
    "        for minute, phone_id, (gx, gy) in gen_grid():\n",
    "            unique_grid_points.add((gx, gy))\n",
    "            series_writer.writerow((str(minute), str(phone_id), str(gx), str(gy)))\n",
    "    mid_x, mid_y = (min_x + max_x)/2, (min_y + max_y)/2\n",
    "    x_distance = distance((min_x, mid_y), (max_x, mid_y))\n",
    "    y_distance = distance((mid_x, min_y), (mid_x, max_y))\n",
    "    print(\"The X values lie between %r and %r - a distance of %r km\" % (min_x, max_x, x_distance))\n",
    "    print(\"The Y values lie between %r and %r - a distance of %r km\" % (min_y, max_y, y_distance))\n",
    "\n",
    "    print(\"%d unique tracked points reduce to %d unique grid points\" % (len(unique_track_points), len(unique_grid_points)))\n",
    "\n",
    "def time_grid_to_bucket(minute, gx, gy):\n",
    "    hour = int(minute / grid_params.TIME_POCKET)\n",
    "    return hashlib.sha1((b'%d:%d,%d' % (hour, gx, gy))).hexdigest()\n",
    "\n",
    "def gen_buckets():\n",
    "    with open(tpg_series_cache_filename, 'r') as csv_file:\n",
    "        series_reader = csv.reader(csv_file)\n",
    "        for row in series_reader:\n",
    "            minute_s, phone_id_s, gx_s, gy_s = row\n",
    "            minute, phone_id, gx, gy = float(minute_s), int(phone_id_s), float(gx_s), float(gy_s)\n",
    "            yield (phone_id, time_grid_to_bucket(minute, gx, gy))\n",
    "\n",
    "pb_series_cache_filename = make_cache_name('phone-bucket', grid_params, 'csv')\n",
    "if not os.path.exists(pb_series_cache_filename):\n",
    "    print (\"Converting to buckets\")\n",
    "    with open(pb_series_cache_filename, 'w') as f:\n",
    "        series_writer = csv.writer(f)\n",
    "        for phone_id, bucket in gen_buckets():\n",
    "            series_writer.writerow((str(phone_id), str(bucket)))\n",
    "\n",
    "# TODO: make these file-based too\n",
    "phone_bucket_map = {}\n",
    "bucket_phone_map = {}\n",
    "\n",
    "def populate_bucket_maps():\n",
    "    print(\"Populating phone<->bucket maps\")\n",
    "    with open(pb_series_cache_filename, 'r') as csv_file:\n",
    "        series_reader = csv.reader(csv_file)\n",
    "        for row in series_reader:\n",
    "            phone_id_s, bucket = row\n",
    "            phone_id = int(phone_id_s)\n",
    "            phone_bucket_map.setdefault(phone_id, set()).add(bucket)\n",
    "            bucket_phone_map.setdefault(bucket, set()).add(phone_id)\n",
    "\n",
    "populate_bucket_maps()\n",
    "print(\"%d phones map to %d buckets\" % (len(phone_bucket_map), len(bucket_phone_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Buckets for a particular phone\n",
    "\n",
    "We now illustrate how having identified a particular phone, we can search for phones with proximity to that phone in the covered period\n",
    "\n",
    "This algorithm needs to be extended to also search adjacent buckets (which need to be recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for phone ID 3482\n",
      "  Found 2116 buckets\n",
      "  Found 941 phones\n",
      "Searching for phone ID 2824\n",
      "  Found 2256 buckets\n",
      "  Found 829 phones\n",
      "Searching for phone ID 3939\n",
      "  Found 2008 buckets\n",
      "  Found 1368 phones\n",
      "Searching for phone ID 1929\n",
      "  Found 2313 buckets\n",
      "  Found 542 phones\n",
      "Searching for phone ID 953\n",
      "  Found 2241 buckets\n",
      "  Found 973 phones\n",
      "Bucket result sizes:\n",
      "{1: 4236,\n",
      " 2: 2032,\n",
      " 3: 1241,\n",
      " 4: 808,\n",
      " 5: 520,\n",
      " 6: 364,\n",
      " 7: 224,\n",
      " 8: 169,\n",
      " 9: 109,\n",
      " 10: 70,\n",
      " 11: 69,\n",
      " 12: 42,\n",
      " 13: 34,\n",
      " 14: 32,\n",
      " 15: 30,\n",
      " 16: 49,\n",
      " 17: 28,\n",
      " 18: 31,\n",
      " 19: 18,\n",
      " 20: 26,\n",
      " 21: 22,\n",
      " 22: 18,\n",
      " 23: 7,\n",
      " 24: 13,\n",
      " 25: 9,\n",
      " 26: 2,\n",
      " 27: 4,\n",
      " 28: 9,\n",
      " 29: 8,\n",
      " 30: 8,\n",
      " 31: 6,\n",
      " 32: 12,\n",
      " 33: 8,\n",
      " 34: 6,\n",
      " 35: 12,\n",
      " 36: 11,\n",
      " 37: 6,\n",
      " 38: 4,\n",
      " 39: 10,\n",
      " 40: 7,\n",
      " 41: 10,\n",
      " 42: 11,\n",
      " 43: 4,\n",
      " 44: 5,\n",
      " 45: 5,\n",
      " 46: 5,\n",
      " 47: 6,\n",
      " 48: 6,\n",
      " 49: 11,\n",
      " 50: 11,\n",
      " 51: 7,\n",
      " 52: 7,\n",
      " 53: 6,\n",
      " 54: 9,\n",
      " 55: 8,\n",
      " 56: 13,\n",
      " 57: 16,\n",
      " 58: 4,\n",
      " 59: 6,\n",
      " 60: 8,\n",
      " 61: 7,\n",
      " 62: 7,\n",
      " 63: 11,\n",
      " 64: 11,\n",
      " 65: 7,\n",
      " 66: 7,\n",
      " 67: 7,\n",
      " 68: 8,\n",
      " 69: 9,\n",
      " 70: 6,\n",
      " 71: 7,\n",
      " 72: 6,\n",
      " 73: 12,\n",
      " 74: 6,\n",
      " 75: 8,\n",
      " 76: 13,\n",
      " 77: 11,\n",
      " 78: 4,\n",
      " 79: 10,\n",
      " 80: 15,\n",
      " 81: 12,\n",
      " 82: 7,\n",
      " 83: 6,\n",
      " 84: 5,\n",
      " 85: 9,\n",
      " 86: 8,\n",
      " 87: 8,\n",
      " 88: 7,\n",
      " 89: 8,\n",
      " 90: 10,\n",
      " 91: 10,\n",
      " 92: 9,\n",
      " 93: 13,\n",
      " 94: 5,\n",
      " 95: 11,\n",
      " 96: 11,\n",
      " 97: 6,\n",
      " 98: 7,\n",
      " 99: 5,\n",
      " 100: 4,\n",
      " 101: 6,\n",
      " 102: 8,\n",
      " 103: 4,\n",
      " 104: 10,\n",
      " 105: 1,\n",
      " 106: 6,\n",
      " 107: 6,\n",
      " 108: 8,\n",
      " 109: 9,\n",
      " 110: 11,\n",
      " 111: 9,\n",
      " 112: 2,\n",
      " 113: 4,\n",
      " 114: 3,\n",
      " 115: 7,\n",
      " 116: 2,\n",
      " 117: 1,\n",
      " 118: 4,\n",
      " 119: 2,\n",
      " 120: 2,\n",
      " 121: 2,\n",
      " 122: 2,\n",
      " 123: 3,\n",
      " 124: 1,\n",
      " 125: 2,\n",
      " 126: 1,\n",
      " 127: 1,\n",
      " 128: 1,\n",
      " 132: 1,\n",
      " 133: 2,\n",
      " 138: 1,\n",
      " 139: 1,\n",
      " 145: 1,\n",
      " 147: 1,\n",
      " 153: 1,\n",
      " 207: 1,\n",
      " 212: 1,\n",
      " 281: 1}\n"
     ]
    }
   ],
   "source": [
    "bucket_result_sizes = []\n",
    "\n",
    "for search_num in range(5):\n",
    "    search_phone = random.randint(0, gen_params.PHONES)\n",
    "\n",
    "    print(\"Searching for phone ID %d\" % search_phone)\n",
    "\n",
    "    buckets = phone_bucket_map[search_phone]\n",
    "\n",
    "    target_phones = set()\n",
    "\n",
    "    print(\"  Found %d buckets\" % len(buckets))\n",
    "\n",
    "    for bucket in buckets:\n",
    "        bucket_phones = bucket_phone_map.get(bucket, [])\n",
    "        bucket_result_sizes.append(len(bucket_phones))\n",
    "        target_phones.update(bucket_phones)\n",
    "\n",
    "    print(\"  Found %d phones\" % len(target_phones))\n",
    "    \n",
    "bucket_len_dist = {size: bucket_result_sizes.count(size) for size in set(bucket_result_sizes)}\n",
    "print(\"Bucket result sizes:\")\n",
    "import pprint\n",
    "pprint.pprint(bucket_len_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
